---
title: "Attention Mechanism in Deep Learning"
date: 2019-07-20T19:51:16-05:00
draft: true
---

What it is: basically a way for a seq2seq model to compute output sequence at each time step by assigning appropriate weights to input sequence at each time step